{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning für Computer Vision  SoSe 2022\n",
    "\n",
    "\n",
    "### Praktikum 11\n",
    "\n",
    "AutoEncoder: Wie gut funktionieren Autoencoder auf dem101-Datensatz?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import matplotlib as mpl\n",
    "# from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fügen Sie hier den Pfad zu dem 101 Ordner ein. Je nach Prozessor/GPU verwenden sie nicht alle Unterordner sondern nur eine Teilmenge.\n",
    "Skalieren Sie die Bilder auf die Größe 50x50 Pixel und konvertieren Sie die Bilder in Grauwertbilder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"101_special\"\n",
    "data_dir = pathlib.Path(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2667\n"
     ]
    }
   ],
   "source": [
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "print(image_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speichern Sie die Bilder und die Labels in die Variablen x und y. Die Bilder werden alle auf die gleiche Größe skaliert und sollen mit der Funktion norm(.) . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "IMG_HEIGHT = 50\n",
    "IMG_WIDTH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(im):\n",
    "    return (im - np.min(im)) / (np.max(im) - np.min(im))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignored = {\".DS_Store\", \"LICENSE.txt\"}\n",
    "categories = [data_dir.name+'/'+x+'/' for x in os.listdir(data_dir) if x not in ignored]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['101_special/airplanes/',\n",
       " '101_special/camera/',\n",
       " '101_special/chair/',\n",
       " '101_special/Faces_easy/',\n",
       " '101_special/Motorbikes/',\n",
       " '101_special/revolver/',\n",
       " '101_special/sunflower/',\n",
       " '101_special/watch/',\n",
       " '101_special/windsor_chair/',\n",
       " '101_special/yin_yang/']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         1.         1.         ... 0.70196078 0.70588235 0.70980392]\n",
      " [1.         1.         1.         ... 0.69803922 0.69803922 0.70588235]\n",
      " [1.         1.         1.         ... 0.69411765 0.69411765 0.69803922]\n",
      " ...\n",
      " [1.         1.         1.         ... 0.67843137 0.67843137 0.6745098 ]\n",
      " [1.         1.         1.         ... 0.67058824 0.6745098  0.66666667]\n",
      " [1.         1.         1.         ... 0.66666667 0.66666667 0.6627451 ]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "counter = 0\n",
    "for eachPath in categories:\n",
    "    labelName = eachPath[12:-1]\n",
    "    # print(labelName)\n",
    "    for eachImageFile in os.listdir(eachPath):\n",
    "        # print(eachPath + eachImageFile)\n",
    "        x.append(norm(cv2.imread(eachPath + eachImageFile, cv2.IMREAD_GRAYSCALE)))\n",
    "        y.append(labelName)\n",
    "\n",
    "print(x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 73784 into shape (50,50)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\basti\\Documents\\Git-Repos\\ML-for-CV-INM1\\Tasks\\Practical_11\\praktikum11_Autoencoder.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/basti/Documents/Git-Repos/ML-for-CV-INM1/Tasks/Practical_11/praktikum11_Autoencoder.ipynb#ch0000012?line=0'>1</a>\u001b[0m fig, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m, figsize \u001b[39m=\u001b[39m (\u001b[39m8\u001b[39m,\u001b[39m4\u001b[39m))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/basti/Documents/Git-Repos/ML-for-CV-INM1/Tasks/Practical_11/praktikum11_Autoencoder.ipynb#ch0000012?line=1'>2</a>\u001b[0m ax[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mimshow(np\u001b[39m.\u001b[39;49mreshape(x[\u001b[39m1\u001b[39;49m], (IMG_HEIGHT, IMG_WIDTH)), cmap\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgray\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/basti/Documents/Git-Repos/ML-for-CV-INM1/Tasks/Practical_11/praktikum11_Autoencoder.ipynb#ch0000012?line=2'>3</a>\u001b[0m ax[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mimshow(np\u001b[39m.\u001b[39mreshape(x[\u001b[39m-\u001b[39m\u001b[39m500\u001b[39m], (IMG_HEIGHT, IMG_WIDTH)), cmap\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgray\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:298\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(a, newshape, order)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_reshape_dispatcher)\n\u001b[0;32m    199\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreshape\u001b[39m(a, newshape, order\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    200\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[39m    Gives a new shape to an array without changing its data.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39m           [5, 6]])\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 298\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39mreshape\u001b[39;49m\u001b[39m'\u001b[39;49m, newshape, order\u001b[39m=\u001b[39;49morder)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 73784 into shape (50,50)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAesAAAD8CAYAAABXV4w2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAPQUlEQVR4nO3dcaidd33H8ffHZp3MVR32CpJEW1k6zdzA7tI5hNmhG2kHyR8OSaBsjmLQWRkogw6Hk/qXkzkQsrmMSVXQGv1jXDCloKsUxGhvqVaTUrlGt6bKGrXzH9Fa9t0f53Q7vd70Psl97rnfE98vuHCe5/w4zyfn5pvPOec+90mqCkmS1NdzdjqAJEl6dpa1JEnNWdaSJDVnWUuS1JxlLUlSc5a1JEnNbVrWST6S5PEk37jA/UnyoSRrSR5Kcv34MSWNwXmWFtOQd9Z3Agee5f6bgH3Tr6PAP209lqRtcifOs7RwNi3rqroP+OGzLDkEfKwmTgEvTPKSsQJKGo/zLC2mXSM8xm7g0Zntc9N931u/MMlRJq/Wed7znvc7r3jFK0Y4vHR5e+CBB75fVUtzOpzzLG2TrczyGGU9WFUdB44DLC8v1+rq6jwPLy2kJP+x0xk24jxLF2crszzG2eCPAXtntvdM90laPM6z1NAYZb0C/On0LNLXAD+qqp/7yEzSQnCepYY2/Rg8ySeBG4Grk5wD/hb4JYCq+jBwErgZWAN+DPz5doWVtDXOs7SYNi3rqjqyyf0FvH20RJK2jfMsLSavYCZJUnOWtSRJzVnWkiQ1Z1lLktScZS1JUnOWtSRJzVnWkiQ1Z1lLktScZS1JUnOWtSRJzVnWkiQ1Z1lLktScZS1JUnOWtSRJzVnWkiQ1Z1lLktScZS1JUnOWtSRJzVnWkiQ1Z1lLktScZS1JUnOWtSRJzVnWkiQ1Z1lLktScZS1JUnOWtSRJzVnWkiQ1Z1lLktScZS1JUnOWtSRJzVnWkiQ1Z1lLktScZS1JUnOWtSRJzVnWkiQ1N6iskxxI8kiStSS3b3D/S5Pcm+TBJA8luXn8qJK2ylmWFtOmZZ3kCuAYcBOwHziSZP+6ZX8DnKiqVwOHgX8cO6ikrXGWpcU15J31DcBaVZ2tqieBu4BD69YU8Pzp7RcA3x0voqSROMvSghpS1ruBR2e2z033zXovcEuSc8BJ4B0bPVCSo0lWk6yeP3/+EuJK2oLRZhmcZ2mexjrB7AhwZ1XtAW4GPp7k5x67qo5X1XJVLS8tLY10aEkjGjTL4DxL8zSkrB8D9s5s75num3UrcAKgqr4EPBe4eoyAkkbjLEsLakhZ3w/sS3JtkiuZnHSysm7NfwKvB0jySiYD7udiUi/OsrSgNi3rqnoKuA24B3iYyZmip5PckeTgdNm7gLck+RrwSeDNVVXbFVrSxXOWpcW1a8iiqjrJ5GST2X3vmbl9BnjtuNEkjc1ZlhaTVzCTJKk5y1qSpOYsa0mSmrOsJUlqzrKWJKk5y1qSpOYsa0mSmrOsJUlqzrKWJKk5y1qSpOYsa0mSmrOsJUlqzrKWJKk5y1qSpOYsa0mSmrOsJUlqzrKWJKk5y1qSpOYsa0mSmrOsJUlqzrKWJKk5y1qSpOYsa0mSmrOsJUlqzrKWJKk5y1qSpOYsa0mSmrOsJUlqzrKWJKk5y1qSpOYsa0mSmrOsJUlqzrKWJKk5y1qSpOYGlXWSA0keSbKW5PYLrHlTkjNJTif5xLgxJY3BWZYW067NFiS5AjgG/CFwDrg/yUpVnZlZsw/4a+C1VfVEkhdvV2BJl8ZZlhbXkHfWNwBrVXW2qp4E7gIOrVvzFuBYVT0BUFWPjxtT0gicZWlBDSnr3cCjM9vnpvtmXQdcl+SLSU4lObDRAyU5mmQ1yer58+cvLbGkSzXaLIPzLM3TWCeY7QL2ATcCR4B/SfLC9Yuq6nhVLVfV8tLS0kiHljSiQbMMzrM0T0PK+jFg78z2num+WeeAlar6WVV9G/gmk4GX1IezLC2oIWV9P7AvybVJrgQOAyvr1vwbk1fiJLmayUdpZ8eLKWkEzrK0oDYt66p6CrgNuAd4GDhRVaeT3JHk4HTZPcAPkpwB7gX+qqp+sF2hJV08Z1laXKmqHTnw8vJyra6u7sixpUWS5IGqWt7pHM/GeZY2t5VZ9gpmkiQ1Z1lLktScZS1JUnOWtSRJzVnWkiQ1Z1lLktScZS1JUnOWtSRJzVnWkiQ1Z1lLktScZS1JUnOWtSRJzVnWkiQ1Z1lLktScZS1JUnOWtSRJzVnWkiQ1Z1lLktScZS1JUnOWtSRJzVnWkiQ1Z1lLktScZS1JUnOWtSRJzVnWkiQ1Z1lLktScZS1JUnOWtSRJzVnWkiQ1Z1lLktScZS1JUnOWtSRJzVnWkiQ1Z1lLktScZS1JUnODyjrJgSSPJFlLcvuzrHtjkkqyPF5ESWNxlqXFtGlZJ7kCOAbcBOwHjiTZv8G6q4C/BL48dkhJW+csS4tryDvrG4C1qjpbVU8CdwGHNlj3PuD9wE9GzCdpPM6ytKCGlPVu4NGZ7XPTff8nyfXA3qr67LM9UJKjSVaTrJ4/f/6iw0raktFmebrWeZbmZMsnmCV5DvBB4F2bra2q41W1XFXLS0tLWz20pBFdzCyD8yzN05CyfgzYO7O9Z7rvaVcBrwK+kOQ7wGuAFU9MkdpxlqUFNaSs7wf2Jbk2yZXAYWDl6Tur6kdVdXVVXVNV1wCngINVtbotiSVdKmdZWlCblnVVPQXcBtwDPAycqKrTSe5IcnC7A0oah7MsLa5dQxZV1Ung5Lp977nA2hu3HkvSdnCWpcXkFcwkSWrOspYkqTnLWpKk5ixrSZKas6wlSWrOspYkqTnLWpKk5ixrSZKas6wlSWrOspYkqTnLWpKk5ixrSZKas6wlSWrOspYkqTnLWpKk5ixrSZKas6wlSWrOspYkqTnLWpKk5ixrSZKas6wlSWrOspYkqTnLWpKk5ixrSZKas6wlSWrOspYkqTnLWpKk5ixrSZKas6wlSWrOspYkqTnLWpKk5ixrSZKas6wlSWrOspYkqblBZZ3kQJJHkqwluX2D+9+Z5EySh5J8PsnLxo8qaaucZWkxbVrWSa4AjgE3AfuBI0n2r1v2ILBcVb8NfAb4u7GDStoaZ1laXEPeWd8ArFXV2ap6ErgLODS7oKruraofTzdPAXvGjSlpBM6ytKCGlPVu4NGZ7XPTfRdyK3D3RnckOZpkNcnq+fPnh6eUNIbRZhmcZ2meRj3BLMktwDLwgY3ur6rjVbVcVctLS0tjHlrSiDabZXCepXnaNWDNY8Deme09033PkOQNwLuB11XVT8eJJ2lEzrK0oIa8s74f2Jfk2iRXAoeBldkFSV4N/DNwsKoeHz+mpBE4y9KC2rSsq+op4DbgHuBh4ERVnU5yR5KD02UfAH4V+HSSryZZucDDSdohzrK0uIZ8DE5VnQROrtv3npnbbxg5l6Rt4CxLi8krmEmS1JxlLUlSc5a1JEnNWdaSJDVnWUuS1JxlLUlSc5a1JEnNWdaSJDVnWUuS1JxlLUlSc5a1JEnNWdaSJDVnWUuS1JxlLUlSc5a1JEnNWdaSJDVnWUuS1JxlLUlSc5a1JEnNWdaSJDVnWUuS1JxlLUlSc5a1JEnNWdaSJDVnWUuS1JxlLUlSc5a1JEnNWdaSJDVnWUuS1JxlLUlSc5a1JEnNWdaSJDVnWUuS1JxlLUlSc5a1JEnNDSrrJAeSPJJkLcntG9z/y0k+Nb3/y0muGT2ppC1zlqXFtGlZJ7kCOAbcBOwHjiTZv27ZrcATVfXrwD8A7x87qKStcZalxTXknfUNwFpVna2qJ4G7gEPr1hwCPjq9/Rng9UkyXkxJI3CWpQW1a8Ca3cCjM9vngN+90JqqeirJj4AXAd+fXZTkKHB0uvnTJN+4lNBzdDXr/gzNdM8H/TN2zwfwGyM9zmizDAs3z4vwfe6esXs+6J/xkmd5SFmPpqqOA8cBkqxW1fI8j3+xumfsng/6Z+yeDyYZdzrDRhZpnrvng/4Zu+eD/hm3MstDPgZ/DNg7s71num/DNUl2AS8AfnCpoSRtC2dZWlBDyvp+YF+Sa5NcCRwGVtatWQH+bHr7T4B/r6oaL6akETjL0oLa9GPw6c+tbgPuAa4APlJVp5PcAaxW1Qrwr8DHk6wBP2Tyj8Bmjm8h97x0z9g9H/TP2D0fjJRxG2d5tIzbqHs+6J+xez7on/GS88UXzZIk9eYVzCRJas6yliSpuW0v6+6XNxyQ751JziR5KMnnk7xsnvmGZJxZ98YklWTuv7owJGOSN02fy9NJPtEpX5KXJrk3yYPT7/XNc873kSSPX+h3lTPxoWn+h5JcP8980wytZ3lgxh2dZ2d5Phkvy3muqm37YnISy7eAlwNXAl8D9q9b8xfAh6e3DwOf2s5Ml5DvD4Bfmd5+2zzzDc04XXcVcB9wCljulhHYBzwI/Np0+8XN8h0H3ja9vR/4zpyfw98Hrge+cYH7bwbuBgK8Bvhyw+/xjs3yRWTcsXl2luea8bKb5+1+Z9398oab5quqe6vqx9PNU0x+N3WehjyHAO9jch3nn8wz3NSQjG8BjlXVEwBV9XizfAU8f3r7BcB355iPqrqPydnXF3II+FhNnAJemOQl80kH9J/lQRl3eJ6d5fllvOzmebvLeqPLG+6+0Jqqegp4+vKG8zAk36xbmbwamqdNM04/QtlbVZ+dZ7AZQ57H64DrknwxyakkB+aWbli+9wK3JDkHnATeMZ9og13s39WdOP5OzvIzjj/VbZ6d5XH8Qs7zXC83usiS3AIsA6/b6SyzkjwH+CDw5h2OspldTD4+u5HJu5n7kvxWVf33ToaacQS4s6r+PsnvMfld41dV1f/sdDCNr+M8O8ujuuzmebvfWXe/vOGQfCR5A/Bu4GBV/XRO2Z62WcargFcBX0jyHSY//1iZ84kpQ57Hc8BKVf2sqr4NfJPJwHfJdytwAqCqvgQ8l8l/CtDFoL+rO3z8nb5Uafd5dpbH8Ys5z9v8Q/ZdwFngWv7/RIDfXLfm7TzzpJQTczwJYEi+VzM5mWHfvHJdbMZ167/A/E9KGfI8HgA+Or19NZOPgF7UKN/dwJunt1/J5GdcmfPzeA0XPiHlj3nmCSlfafg93rFZvoiMOzbPzvJcM1528zyPwDczeeX1LeDd0313MHlVC5NXPJ8G1oCvAC+f8xO6Wb7PAf8FfHX6tTLPfEMyrls79wEf+DyGyUd8Z4CvA4eb5dsPfHE6+F8F/mjO+T4JfA/4GZN3LrcCbwXeOvP8HZvm/3rT7/GOzvLAjDs6z87y3DJedvPs5UYlSWrOK5hJktScZS1JUnOWtSRJzVnWkiQ1Z1lLktScZS1JUnOWtSRJzf0vW55tx3DGrcEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize = (8,4))\n",
    "ax[0].imshow(np.reshape(x[1], (IMG_HEIGHT, IMG_WIDTH)), cmap='gray')\n",
    "ax[1].imshow(np.reshape(x[-500], (IMG_HEIGHT, IMG_WIDTH)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide the data into training and test data\n",
    "test_idx = random.sample(range(0, x.shape[0]), int(x.shape[0]*0.1))\n",
    "x_test = x[test_idx]\n",
    "x_train = np.delete(x, test_idx, 0)\n",
    "y_test = y[test_idx]\n",
    "y_train = np.delete(y, test_idx, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize = (8,4))\n",
    "ax[0].imshow(np.reshape(x_test[1], (IMG_HEIGHT, IMG_WIDTH)), cmap='gray')\n",
    "ax[1].imshow(np.reshape(x_test[11], (IMG_HEIGHT, IMG_WIDTH)), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alle layer werden gleichzeitig trainiert\n",
    "\n",
    "Implementieren Sie den stacked autoencoder von Folie 11. Als Metrik wird die rounded accuracy benutzt. Für die Visualisierung der Daten können Sie die Funktion show_reconstructions verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rounded_accuracy(y_true, y_pred):\n",
    "    return keras.metrics.binary_accuracy(tf.round(y_true), tf.round(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_reconstructions(model, images=x_test, n_images=5):\n",
    "    reconstructions = model.predict(images[:n_images])\n",
    "    fig = plt.figure(figsize=(n_images * 1.5, 3))\n",
    "    for image_index in range(n_images):\n",
    "        plt.subplot(2, n_images, 1 + image_index)\n",
    "        plot_image(images[image_index])\n",
    "        plt.subplot(2, n_images, 1 + n_images + image_index)\n",
    "        plot_image(reconstructions[image_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image):\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_reconstructions(stacked_ae, images=x_test[7:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualsierung der Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benutzen Sie PCA und tSNE um die gelernten Codierungen zu visualisieren.\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_new = stacked_encoder.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using PCA\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using tSNE\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisieren Sie einzelne Datenpukte von den unterschiedlichen Clustern.\n",
    "Siehe Beispiel: https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Für PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Für tSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generieren von neuen Beispielen\n",
    "\n",
    "Benutzen sie einen variational autoencoder, um neue Daten zu generieren (s. Folien 25-27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "class Sampling(keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        mean, log_var = inputs\n",
    "        return K.random_normal(tf.shape(log_var)) * K.exp(log_var / 2) + mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_reconstructions(variational_ae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolieren sie die gelernten Codierungen, um neue Beispiel zu erzeugen. Sie können zum Anzeigen der Bilder die Funktion plot_multiple_images benutzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiple_images(images, n_cols=None):\n",
    "    n_cols = n_cols or len(images)\n",
    "    n_rows = (len(images) - 1) // n_cols + 1\n",
    "    if images.shape[-1] == 1:\n",
    "        images = np.squeeze(images, axis=-1)\n",
    "    plt.figure(figsize=(n_cols, n_rows))\n",
    "    for index, image in enumerate(images):\n",
    "        plt.subplot(n_rows, n_cols, index + 1)\n",
    "        plt.imshow(image, cmap=\"gray\")\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiple_images(images, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Adversarial Networks\n",
    "Implementieren sie die GAN-Architektur aus der Vorlesung, Folie 37. Zum Anzeogen der Zwischenergebnisse können Sie die Funktion train_gan benutzen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(gan, dataset, batch_size, codings_size, n_epochs=50):\n",
    "    generator, discriminator = gan.layers\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, n_epochs))              # not shown in the book\n",
    "        for X_batch in dataset:\n",
    "            # phase 1 - training the discriminator\n",
    "            noise = tf.random.normal(shape=[batch_size, codings_size])\n",
    "            generated_images = generator(noise)\n",
    "            X_fake_and_real = tf.concat([generated_images, np.squeeze(X_batch)], axis=0)\n",
    "            y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)\n",
    "            discriminator.trainable = True\n",
    "            discriminator.train_on_batch(X_fake_and_real, y1)\n",
    "            # phase 2 - training the generator\n",
    "            noise = tf.random.normal(shape=[batch_size, codings_size])\n",
    "            y2 = tf.constant([[1.]] * batch_size)\n",
    "            discriminator.trainable = False\n",
    "            gan.train_on_batch(noise, y2)\n",
    "        plot_multiple_images(generated_images, 8)                     # not shown\n",
    "        plt.show()           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gan(gan, dataset, batch_size, codings_size, n_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN mit CNNs \n",
    "Wie sieht das Ergebnis aus, wenn Sie CNNs verwenden? (s. Folie 37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
